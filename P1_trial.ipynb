{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T17:22:29.856194Z",
     "start_time": "2019-08-27T17:22:28.504835Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('news_chinese.csv')\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 获得所有表示“说”的意思的单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T16:56:04.396204Z",
     "start_time": "2019-08-27T16:56:04.197205Z"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "LTP_DATA_DIR = '.\\\\ltp_data_v3.4.0\\\\'  # ltp模型目录的路径\n",
    "cws_model_path = os.path.join(LTP_DATA_DIR, 'cws.model')  # 分词模型路径，模型名称为`cws.model`\n",
    "\n",
    "from pyltp import Segmentor\n",
    "segmentor = Segmentor()  # 初始化实例\n",
    "segmentor.load(cws_model_path)  # 加载模型\n",
    "\n",
    "def cut_words(text):\n",
    "    words = segmentor.segment(text)  # 分词\n",
    "    return words\n",
    "#     segmentor.release()  # 释放模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T17:20:02.262676Z",
     "start_time": "2019-08-27T17:20:02.257707Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyltp import SentenceSplitter\n",
    "\n",
    "def split_sentence(text):\n",
    "    '''\n",
    "    split text into single sentence without '\\n'\n",
    "    '''\n",
    "    sentences = []\n",
    "    try:\n",
    "        sentence_list = text.split('\\\\n')\n",
    "        for sentence in sentence_list:\n",
    "            splited_sentence = SentenceSplitter.split(sentence)  # 分句\n",
    "            sentences.extend(list(splited_sentence))\n",
    "    except Exception:\n",
    "        print('split_sentence exception:' + str(text))\n",
    "        print(Exception)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T17:00:52.305342Z",
     "start_time": "2019-08-27T17:00:52.301341Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    preprocess text, drop number, blank, stopwords\n",
    "    return segments list\n",
    "    \"\"\"\n",
    "#     stopwords=pd.read_csv('.\\\\stopwords.txt',index_col=False,quoting=3,sep=\"\\t\",names=['stopword'], encoding='utf-8')\n",
    "#     stopwords=stopwords['stopword'].values\n",
    "    if len(text) == 1:\n",
    "        return ''\n",
    "    \n",
    "    try:\n",
    "        segs = list(cut_words(text))\n",
    "#         segs = [v for v in segs if not str(v).isdigit()]#去数字\n",
    "        segs = list(filter(lambda x:x.strip(), segs)) #去左右空格\n",
    "#         segs = list(filter(lambda x:len(x)>1, segs))#长度为1的字符\n",
    "#         segs = list(filter(lambda x:x not in stopwords, segs)) #去掉停用词\n",
    "    except Exception:\n",
    "        print('preprocess_text exception:' + str(text))\n",
    "        print(Exception)\n",
    "    return segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T17:25:20.492826Z",
     "start_time": "2019-08-27T17:22:50.281775Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "preprocess and write corpus to file for future use\n",
    "'''\n",
    "with open('corpus.txt', 'w', encoding='utf-8') as f:\n",
    "    for graph in data['content'].values:\n",
    "        sentences = split_sentence(graph)  # 分句\n",
    "        for sentence in sentences:\n",
    "            words = preprocess_text(sentence)\n",
    "            if words == '':\n",
    "                continue\n",
    "            f.write(' '.join(words))\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T17:44:09.938215Z",
     "start_time": "2019-08-27T17:41:54.444315Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\py36\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "E:\\Anaconda3\\envs\\py36\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "corpus = LineSentence('corpus.txt')\n",
    "'''\n",
    "LineSentence(inp)：格式简单：一句话=一行; 单词已经过预处理并被空格分隔。\n",
    "size：是每个词的向量维度； \n",
    "window：是词向量训练时的上下文扫描窗口大小，窗口为5就是考虑前5个词和后5个词； \n",
    "min-count：设置最低频率，默认是5，如果一个词语在文档中出现的次数小于5，那么就会丢弃； \n",
    "workers：是训练的进程数（需要更精准的解释，请指正），默认是当前运行机器的处理器核数。这些参数先记住就可以了。\n",
    "sg ({0, 1}, optional) – 模型的训练算法: 1: skip-gram; 0: CBOW\n",
    "alpha (float, optional) – 初始学习率\n",
    "iter (int, optional) – 迭代次数，默认为5\n",
    "'''\n",
    "model = Word2Vec(sentences=corpus, size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "model.save(\".\\\\word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.20866792,  0.7205171 , -1.0260113 , -0.06374749,  0.19556347,\n",
       "       -0.10682994, -0.5851775 , -0.01481899,  0.3670453 ,  0.24029326,\n",
       "        0.5949725 ,  0.28039786, -0.06872837, -0.33250424, -0.1844618 ,\n",
       "       -0.5172169 ,  0.34398785, -0.58655286, -0.1243936 , -0.75491464,\n",
       "        0.09888597,  0.40916127,  0.10244063,  0.7225233 ,  0.32497236,\n",
       "        0.56063825,  0.8444974 , -0.3222012 ,  0.34923565,  0.27028653,\n",
       "       -0.51589775, -0.5600994 ,  0.2264611 , -0.385016  , -0.4579577 ,\n",
       "        0.08523443,  0.64386386, -0.10020302, -0.01174884, -0.51918715,\n",
       "       -0.01156036, -0.22992441,  0.77415377,  0.05849807, -0.2401069 ,\n",
       "       -0.00274309, -0.7530591 ,  0.07110906, -0.13691097,  0.1307326 ,\n",
       "       -0.49360964, -0.21363465,  0.7133124 ,  0.357119  ,  0.47492936,\n",
       "       -0.23782545,  1.0265316 , -0.13638072, -0.27368957, -0.03718648,\n",
       "        0.7090336 , -0.76925814, -0.48834   ,  0.71585935,  0.19875243,\n",
       "       -0.85786104,  0.28460357, -0.29654166, -0.14113064,  0.8782624 ,\n",
       "       -0.11440696,  0.20456931, -0.24379306,  0.80188143,  0.19253168,\n",
       "        0.04655569, -0.82315147,  0.13960767,  0.06716272, -0.46724465,\n",
       "       -0.02533841, -0.47917455, -0.6083708 ,  0.75265986,  0.12438376,\n",
       "       -0.4198804 , -0.04766526,  0.38098052,  0.37672096,  0.706703  ,\n",
       "       -0.44198063, -0.8817988 , -0.20643437,  0.16330408,  0.51761544,\n",
       "       -0.37391335,  0.09321438,  0.38813108, -0.32184866, -0.31084642],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['说']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 找相似词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embedding Tutorial: word2vec using Gensim\n",
    "\n",
    "https://www.guru99.com/word-embedding-word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T10:01:10.827019Z",
     "start_time": "2019-09-11T10:01:04.803328Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\py36\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "E:\\Anaconda3\\envs\\py36\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "load gensim word2vec model\n",
    "'''\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\".\\\\word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T10:01:16.071752Z",
     "start_time": "2019-09-11T10:01:16.059747Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223587"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(model.wv.vocab)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T18:07:42.600970Z",
     "start_time": "2019-09-09T18:07:42.228263Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Soft\\Anaconda3\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('表示', 0.8741297125816345), ('告诉', 0.8271186351776123), ('指出', 0.788396954536438), ('认为', 0.7727850675582886), ('介绍', 0.7356007695198059), ('说道', 0.7067561745643616), ('看来', 0.6960127353668213), ('写道', 0.6913279891014099), ('话', 0.6870427131652832), ('强调', 0.6705794334411621), ('坦言', 0.6692733764648438), ('提到', 0.6580647230148315), ('称', 0.6515368223190308), ('透露', 0.6492000222206116), ('举例', 0.6212161779403687), ('形容', 0.611984372138977), ('证实', 0.6084652543067932), ('得知', 0.606695294380188), ('说法', 0.6029568910598755), ('问', 0.6021037101745605)]\n"
     ]
    }
   ],
   "source": [
    "similar_words = model.most_similar('说', topn=20)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T18:07:48.504598Z",
     "start_time": "2019-09-09T18:07:48.500620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide the similarity between these two words:\n",
      "0.3628039284301186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Soft\\Anaconda3\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "similarity_two_words = model.similarity('说','讲')\n",
    "print(\"Please provide the similarity between these two words:\")\n",
    "print(similarity_two_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T18:07:52.778618Z",
     "start_time": "2019-09-09T18:07:52.764648Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('表示', 0.8741297125816345), ('告诉', 0.8271186351776123), ('指出', 0.788396954536438), ('认为', 0.7727850675582886), ('介绍', 0.7356007695198059), ('说道', 0.7067561745643616), ('看来', 0.6960127353668213), ('写道', 0.6913279891014099), ('话', 0.6870427131652832), ('强调', 0.6705794334411621), ('坦言', 0.6692733764648438), ('提到', 0.6580647230148315), ('称', 0.6515368223190308), ('透露', 0.6492000222206116), ('举例', 0.6212161779403687), ('形容', 0.611984372138977), ('证实', 0.6084652543067932), ('得知', 0.606695294380188), ('说法', 0.6029568910598755), ('问', 0.6021037101745605)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Soft\\Anaconda3\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "similar = model.similar_by_word('说', topn=20)\n",
    "print(similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T10:01:31.634775Z",
     "start_time": "2019-09-11T10:01:31.629788Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_similar_words(words, max_size, model):\n",
    "    similar_words = []\n",
    "    seen = []\n",
    "    unseen = ['说']\n",
    "    while len(similar_words) < max_size and len(unseen) != 0:\n",
    "        unseen_word = unseen.pop(0)\n",
    "        if unseen_word in similar_words:\n",
    "            continue\n",
    "        similars = [w for w, p in model.most_similar(unseen_word, topn=20)]\n",
    "        unseen.extend(similars)\n",
    "        similar_words.append(unseen_word)\n",
    "        seen.append(unseen_word)\n",
    "    return similar_words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T10:04:16.005670Z",
     "start_time": "2019-09-11T10:01:42.475118Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\vick_\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.100 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "words = []\n",
    "with open('corpus.txt', 'r', encoding='utf-8') as rf:\n",
    "    for line in rf.readlines():\n",
    "        words.extend(list(jieba.cut(line)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T18:12:44.824969Z",
     "start_time": "2019-09-09T18:12:44.819997Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33172819"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T10:07:29.321168Z",
     "start_time": "2019-09-11T10:07:28.662679Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['说',\n",
       " '表示',\n",
       " '告诉',\n",
       " '指出',\n",
       " '认为',\n",
       " '说道',\n",
       " '坦言',\n",
       " '介绍',\n",
       " '看来',\n",
       " '透露',\n",
       " '写道',\n",
       " '举例',\n",
       " '强调',\n",
       " '深有感触',\n",
       " '称',\n",
       " '话',\n",
       " '直言',\n",
       " '说法',\n",
       " '提到',\n",
       " '笑言']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words = find_similar_words(words, 20, model)\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 使用 NER，Dependency Parsing等对句子形式进行解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T10:27:36.584986Z",
     "start_time": "2019-09-11T10:27:36.580987Z"
    }
   },
   "outputs": [],
   "source": [
    "similar_words = ['说',\n",
    " '表示',\n",
    " '告诉',\n",
    " '指出',\n",
    " '认为',\n",
    " '说道',\n",
    " '坦言',\n",
    " '介绍',\n",
    " '看来',\n",
    " '透露',\n",
    " '写道',\n",
    " '举例',\n",
    " '强调',\n",
    " '深有感触',\n",
    " '称',\n",
    " '话',\n",
    " '直言',\n",
    " '说法',\n",
    " '提到',\n",
    " '笑言']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T10:27:38.927594Z",
     "start_time": "2019-09-11T10:27:37.599359Z"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "LTP_DATA_DIR = '.\\\\ltp_data_v3.4.0\\\\'  # ltp模型目录的路径\n",
    "pos_model_path = os.path.join(LTP_DATA_DIR, 'pos.model')  # 词性标注模型路径，模型名称为`pos.model`\n",
    "ner_model_path = os.path.join(LTP_DATA_DIR, 'ner.model')  # 命名实体识别模型路径，模型名称为`pos.model`\n",
    "par_model_path = os.path.join(LTP_DATA_DIR, 'parser.model')  # 依存句法分析模型路径，模型名称为`parser.model`\n",
    "srl_model_path = os.path.join(LTP_DATA_DIR, 'pisrl_win.model')  # 语义角色标注模型目录路径，模型目录为`srl`。注意该模型路径是一个目录，而不是一个文件。\n",
    "\n",
    "from pyltp import Postagger\n",
    "postagger = Postagger() # 初始化实例\n",
    "postagger.load(pos_model_path)  # 加载模型\n",
    "\n",
    "from pyltp import NamedEntityRecognizer\n",
    "recognizer = NamedEntityRecognizer() # 初始化实例\n",
    "recognizer.load(ner_model_path)  # 加载模型\n",
    "\n",
    "from pyltp import Parser\n",
    "parser = Parser() # 初始化实例\n",
    "parser.load(par_model_path)  # 加载模型\n",
    "\n",
    "from pyltp import SementicRoleLabeller\n",
    "labeller = SementicRoleLabeller() # 初始化实例\n",
    "labeller.load(srl_model_path)  # 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T10:27:46.272088Z",
     "start_time": "2019-09-11T10:27:46.269075Z"
    }
   },
   "outputs": [],
   "source": [
    "def contain_keywords(keywords, words):\n",
    "    '''\n",
    "    return key word in words, return empty if not match\n",
    "    '''\n",
    "    for keyword in keywords:\n",
    "        if keyword in words:\n",
    "            return keyword\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T10:28:33.764645Z",
     "start_time": "2019-09-11T10:27:50.181802Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "maxsize = 100\n",
    "count = 0\n",
    "\n",
    "subs = []\n",
    "says = []\n",
    "points = []\n",
    "\n",
    "with open('corpus.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        words = line.split(' ')\n",
    "        keyword = contain_keywords(similar_words, words) # 句子是否包含“说”的同义词\n",
    "        if keyword == '':\n",
    "            continue\n",
    "#         print('%d:%s' % (words.index(keyword), keyword))\n",
    "        postags = postagger.postag(words)  # 词性标注\n",
    "        netags = recognizer.recognize(words, postags)  # 命名实体识别\n",
    "        arcs = parser.parse(words, postags)  # 句法分析\n",
    "        \n",
    "        # 打印词性标注，命名实体识别，句法分析结果(可注释)\n",
    "#         print('序号' + '\\t' + '词' + '\\t' + '词性' + '\\t' + '命名实体' + '\\t' + '依存句法')\n",
    "#         print('--------------------------------------------')\n",
    "#         word_index = 0\n",
    "#         for w,p,n,a in zip(words, postags, netags, (\"%d:%s\" % (arc.head, arc.relation) for arc in arcs)):\n",
    "#             print(str(word_index) + '\\t' + w + '\\t' + p + '\\t' + n + '\\t' + a)\n",
    "#             word_index += 1\n",
    "        \n",
    "        # arcs 使用依存句法分析的结果\n",
    "        roles = labeller.label(words, postags, arcs)  # 语义角色标注\n",
    "        \n",
    "        # 打印依存句法分析结果(可注释)\n",
    "#         for role in roles:\n",
    "#             print(role.index, \"\".join(\n",
    "#                 [\"%s:(%d,%d)\" % (arg.name, arg.range.start, arg.range.end) for arg in role.arguments]))\n",
    "        \n",
    "        for role in roles:\n",
    "            if words.index(keyword) == role.index:\n",
    "                for arg in role.arguments:\n",
    "                    if arg.name == 'A0':\n",
    "                        sub = ''.join(words[arg.range.start:arg.range.end+1])\n",
    "                        point = ''.join(words[role.index+1:])\n",
    "#                         print(\"观点主体: \" + sub)\n",
    "#                         print(\"观点内容: \" + point)\n",
    "                        subs.append(sub)\n",
    "                        says.append(keyword)\n",
    "                        points.append(point)\n",
    "        \n",
    "#         print('\\n======================================================================\\n')\n",
    "        count += 1\n",
    "        if count == maxsize:\n",
    "            break\n",
    "\n",
    "postagger.release()  # 释放模型\n",
    "recognizer.release()  # 释放模型\n",
    "parser.release()  # 释放模型\n",
    "labeller.release()  # 释放模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T10:28:48.248082Z",
     "start_time": "2019-09-11T10:28:48.240087Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "来自广州的茶茶\t说\t。\n",
      "\n",
      "她\t告诉\t父母这次出来是到德国看朋友，住宿和球票的花费都是平时做家教挣的，父母支援了机票钱，从香港到杜塞尔多夫往返只有三千多元人民币，让其他内地球迷非常羡慕。\n",
      "\n",
      "熊老师\t说\t，这次有了德国的签证，以后去日本看比赛签证就容易多了。\n",
      "\n",
      "井井\t说\t。\n",
      "\n",
      "井井\t说\t。\n",
      "\n",
      "茶茶\t说\t。\n",
      "\n",
      "她\t写道\t。\n",
      "\n",
      "哈佛大学招生部门\t表示\t要审核他们的入学情况。\n",
      "\n",
      "哈佛大学发言人理查德·戴恩\t说\t，校方不会公开讨论申请入学者情况。\n",
      "\n",
      "约60%的受访者\t表示\t，他们对马克龙的表现感到满意。\n",
      "\n",
      "甘肃省交通运输厅\t介绍\t，此次集中开建35个重点交通建设项目，在进一步完善路网结构的同时，有助于促进甘肃经济运行趋稳向好、确保完成全年固定资产投资目标任务。\n",
      "\n",
      "韩国国防部官员\t说\t，针对“萨德”在韩部署事宜，国防部方面已准备重新进行一场大规模的环境评估。\n",
      "\n",
      "韩国国防部官员\t透露\t，青瓦台要求彻查“萨德”环境评估过程后，国防部已着手开展准备工作，将进行一场大规模的全面评估，预计可能耗时一年左右。\n",
      "\n",
      "这名官员\t透露\t，目前，针对“萨德”用地的小规模环境评估正在进行中，预计将于本月底结束。\n",
      "\n",
      "青瓦台方面\t表示\t，经调查发现，国防部在“萨德”环境评估过程中存在“猫腻”，试图“大事化小”，绕过相关法律规定。\n",
      "\n",
      "青瓦台方面\t表示\t，总统文在寅获悉除目前已在韩部署的两台“萨德”系统移动发射架外，另有4台发射架已经秘密运抵韩国，感到“非常震惊”，要求彻查。\n",
      "\n",
      "青瓦台公报首席秘书尹永灿\t说\t，调查已确认，瞒报事件主要责任人是国防部政策室长魏昇镐。\n",
      "\n",
      "青瓦台方面\t表示\t，将就此事展开进一步调查，以查明国防部长官韩民求和前总统朴槿惠时期的国家安保室长金宽镇是否与瞒报事件有关联。\n",
      "\n",
      "肖云峰\t说\t，除了水果玉米，他还种植了爆米花玉米等，收入也很可观。\n",
      "\n",
      "肖云峰\t认为\t，大学生职业农民最大的优势在于对国家惠农政策的理解，以及对农业技术的掌控。\n",
      "\n",
      "沈丘县农牧局局长童玉体\t说\t：“别人种玉米，他种水果玉米；\n",
      "\n",
      "王晓康\t说\t，苗圃种植园经过整合会成为休闲农庄，不仅可以通过优质别样的苗木促进农业增收，也有利于农村生态，通过生态旅游创收。\n",
      "\n",
      "王文一\t说\t。\n",
      "\n",
      "周口市农业局科教科科长刘民乾\t说\t：“能够吸引众多大学生回归田野进行创业，是因为土地效益的巨大吸引力，而土地效益源于他们对市场的敏锐、对技术的把握以及科学的经营管理，如此，才能使平凡的土地生产出‘高大上’的农产品。”\n",
      "\n",
      "河南省社会科学院长期关注农业问题的专家刘刚\t表示\t，“大学生创业潮”有利于促进农民增收，同时激活农业创新和农村发展潜力。\n",
      "\n",
      "山西省金融办副主任张炯玮\t说\t，《若干意见》为山西省基金业发展明确了四大目标。\n",
      "\n",
      "张炯玮\t说\t，为吸引省外优秀投资机构和投资人才落户山西，《若干意见》提出了具体的优惠措施。\n",
      "\n",
      "当地大学生卡纳特\t告诉\t记者，他认为上合峰会和世博会将提升阿斯塔纳的全球知名度，吸引更多游客，也有助于增进各方交流理解。\n",
      "\n",
      "天津航空副董事长兼总裁吴浩\t介绍\t，未来，天津航空将抓紧“一带一路”建设向纵深发展的机遇，设立重庆运营基地，进一步打造泛欧和泛亚走廊，将“走出去”这块蛋糕越做越大。\n",
      "\n",
      "蔡华\t指出\t，家长不要给考生补充过多蛋白质、脂肪类食物，考生的肠胃不一定习惯，反而容易导致腹泻、腹痛等现象。\n",
      "\n",
      "陆和平\t说\t，“看到我的劳动成果能帮助农民止损，特别开心。”\n",
      "\n",
      "伊拉克总理\t表示\t遗憾\n",
      "\n",
      "黑龙江省人社部门\t介绍\t，此次阶段性降低失业保险费率，总费率由1.5%降至1%，其中单位缴费率由1%调整为0.5%，个人缴费率保持0.5%不变，降低失业保险费率期限为2017年1月1日至2018年4月30日。\n",
      "\n",
      "57%的居民\t认为\t家政服务质量较差。\n",
      "\n",
      "北京市民\t认为\t家政服务业中存在的主要问题是缺乏统一服务标准、市场监管不到位、服务价格混乱和服务质量较差，持这些看法的调查对象分别占66%、62%、61%和57%。\n",
      "\n",
      "北京市政协委员\t认为\t，家政服务的重要性不言而喻，尤其是进入老龄化社会和“二胎时代”，家政服务不仅需求大，而且人们对服务质量有新的要求。\n",
      "\n",
      "于雪鹰\t说\t。\n",
      "\n",
      "樊澄\t介绍\t，在北京4000余家家政服务企业中，注册资金在10万元以上的仅800家左右。\n",
      "\n",
      "专家\t指出\t，北京应加大家政服务龙头型示范企业培育，支持和鼓励示范企业通过资源整合或加盟连锁等方式，增加服务网点、完善服务网络，实现规模化、连锁化、品牌化经营；\n",
      "\n",
      "在此间出席第１２届中欧工商峰会的陈峰\t说\t：“海航集团的扩张都是紧紧围绕自身业务的上下游产业进行，看中的是国外优质资产，利用的是全球资本。\n",
      "\n",
      "陈峰\t认为\t，一方面，当今世界正面临经济低位徘徊、逆全球化暗流涌动、传统欧美模式受到前所未有的冲击等诸多挑战；\n",
      "\n",
      "陈峰\t表示\t，海航集团有三大目标：一是打造一个品牌和发展模式在业界都堪称经典的案例；\n",
      "\n",
      "陈峰\t说\t。\n",
      "\n",
      "陈峰\t说\t：“我觉得海航继承了中国的文化自信，用文化来凝聚全球人才，而且实践证明，这是完全可以做到的。”\n",
      "\n",
      "他\t说\t：“文化先导是海航并购、整合的一大特色。\n",
      "\n",
      "陈峰\t强调\t，在“一带一路”倡议的指引下，海航集团将重点打造国际化民族品牌，在“走出去”的同时，把优秀的经验、先进的理念、高质量的服务等好的东西“拿回来”，以提升自身的国际竞争力，更好地助力“一带一路”建设。\n",
      "\n",
      "他\t说\t，在并购过程中，公司打通了国际、国内资本市场，把国外优质资产引入国内市场，并通过定向增发等方式，实现降低负债率和增加资产的目的。\n",
      "\n",
      "索尔海姆\t说\t，目前已经有２０多个国家承诺减少塑料垃圾，但要想大幅减少每年进入海洋的８００万吨塑料垃圾，我们还需要更多国家做出承诺。\n",
      "\n",
      "该机构\t指出\t，化妆品中的塑料微粒成分和一次性塑料制品的过量使用是海洋垃圾的主要来源，“清洁海洋”运动的目标是在２０２２年前消除这类垃圾。\n",
      "\n",
      "浙江省工商局相关负责人\t表示\t，只有管得住，才能放得开，在登记审批上倡导“最多跑一次”，在事中事后监管上探索“最多查一次”。\n",
      "\n",
      "、中国工程院主席团名誉主席徐匡迪\t指出\t，雄安新区的发展绝不应被房地产商绑架，要在体制机制等方面做变化，为中国今后城市发展尝试走一条新的路子。\n",
      "\n",
      "徐匡迪\t介绍\t了雄安新区规划建设的一些思路。\n",
      "\n",
      "徐匡迪\t介绍\t，新区规划的原则是水城相融、蓝绿互映的生态宜居城市，绿地要超过50%。\n",
      "\n",
      "他\t说\t，白洋淀复杂的水陆情况对新区规划既是有利因素也是挑战，其中最大的挑战是淀中村及堤上村。\n",
      "\n",
      "徐匡迪\t说\t，新区是创新经济的载体，是先进科技文化的结晶，更是和谐宜居的人类家园。\n",
      "\n",
      "他\t说\t，在城市规划方面，新区建设一大亮点是要建设21世纪的地下管廊式基础设施，把城市交通、水电气、城市灾害防护系统等都放到地下，把地面让给绿化和人的行走。\n",
      "\n",
      "徐匡迪\t指出\t，目前新区规划还在深入研究，正在探讨几个重大问题：一是如何做到理水营城，真正形成水城融、蓝绿汇、天人合；\n",
      "\n",
      "晋江市公安局经侦大队三中队副中队长陈晓山\t说\t。\n",
      "\n",
      "福建泉州南安市的紫菜养殖户李强荣\t表示\t，近１０年来紫菜价格波动不大，基本上在１万元／吨左右，很少有滞销，但“塑料紫菜”谣言出来后，原料收购价格从８万元／吨降到不足５万元／吨。\n",
      "\n",
      "李强荣\t说\t。\n",
      "\n",
      "王某祥\t认为\t自己吃到了“塑料做的假紫菜”，便联系生产企业进行维权。\n",
      "\n",
      "王某祥\t说\t，为了能要上高价，他指挥自己的员工拍摄“塑料紫菜”视频并上传到网络，同时威胁生产企业，要是不给钱解决，他将继续在网上大量转发。\n",
      "\n",
      "福建省水产研究所原副所长吴成业\t说\t。\n",
      "\n",
      "公安部网络安全保卫局副局长张宏业\t介绍\t，目前在网上传播食品安全谣言的人一部分是为博取眼球、赚取热度，置科学常识和实际情况于不顾，偷换概念、歪曲事实，也有极个别人是为谋取不法利益等目的对食品生产销售企业、在售食品编造谣言，有意抹黑。\n",
      "\n",
      "晋江市公安局经侦大队大队长杨剑扬\t表示\t，一些造谣者善于使用吸引眼球的语言和图片，满足受众猎奇心理。\n",
      "\n",
      "陈晓山\t表示\t，由于取证难、溯源难，侦破食品安全网络谣言案件并非易事。\n",
      "\n",
      "专家\t表示\t，治理食品安全网络谣言，维护社会公共秩序和良好舆论环境，需要全社会共同努力。\n",
      "\n",
      "公安部网安局\t表示\t，将继续加大对食品安全谣言的打击整治力度。\n",
      "\n",
      "张宏业\t说\t，广大群众也应注意理性、文明上网，注意辨别各类网上不实信息，自觉抵制谣言，不造谣、不信谣、不传谣，发现谣言及时举报。\n",
      "\n",
      "国家食品药品监督管理总局新闻宣传司司长颜江瑛\t强调\t，食药监总局将推动建立社会多元主体共治谣言的长效机制，让政府部门、专业机构人士、相关企业、新闻媒体和公众形成合力，“五位一体”击碎谣言，共同织密甄别、抵制谣言的免疫网。\n",
      "\n",
      "颜江瑛\t说\t，未来还将继续加大此类信息公开力度，增加重点信息展示强度，强化信息查询服务功能。\n",
      "\n",
      "黑龙江省高院副院长史景山\t介绍\t，２０１６年全省法院新收一审非法集资犯罪案件７８件，分别是２０１５年、２０１４年的１．４４倍、１．８６倍。\n",
      "\n",
      "黑龙江省高院刑二庭庭长陈永刚\t介绍\t，非法集资犯罪手段不断翻新升级，除了常见的种植业、养殖业等领域外，非法集资犯罪向金融投资、委托理财、电子商务等领域渗透，往往冠以“Ｐ２Ｐ网络借贷”“财富管理”“互联网金融”等名义，采取债权、股权、债转股等诸多形式，混淆非法集资与合法融资的界限，让群众很难甄别。\n",
      "\n",
      "匿名的安全部门人士\t告诉\t记者，一名袭击者当晚在安巴尔省希特镇中心一处市场引爆了身上的爆炸物，由于事发时市场内人员众多，袭击造成的伤亡人数可能将继续上升。\n",
      "\n",
      "贫困户丁罗香\t告诉\t记者：“在车间里做制衣加工和彩灯制作，一年下来也能有个万把块钱收入。”\n",
      "\n",
      "康富达仿真花卉公司负责人赖垂龙\t说\t：“像我们这种加工型企业需要大量劳动力，自从把车间建到村里之后，招工不再那么困难，生产成本也下降了。”\n",
      "\n",
      "赣州市扶贫办副主任钟小春\t认为\t，扶贫车间能够提高贫困群众的组织化程度，通过持续稳定的就业，还可以提升他们的综合素质。\n",
      "\n",
      "徐志坚\t说\t。\n",
      "\n",
      "赛事主办方香港业余田径总会主席关祺\t表示\t，香港马拉松以服务好本地民众为第一要旨，赛事完全是市场运作，加上特区政府的鼎力支持，才得以吸引越来越多香港市民参与。\n",
      "\n",
      "大型体育活动事务委员会主席郭志梁\t说\t：“‘Ｍ’品牌活动展示香港举办世界级体坛盛事的实力，也提升香港作为亚洲盛事之都的地位。\n",
      "\n",
      "郭志梁\t表示\t，由特区政府主导并得到社会各界支持，香港打造“Ｍ”品牌赛事大大提高了民众参与体育的热情。\n",
      "\n",
      "美军中央司令部\t说\t，由超过６０名武装人员和坦克等装备组成的亲叙政府武装力量当天无视警告进入叙南部坦夫山一处“冲突降级区”，对驻守该地的联军及其支持的叙利亚反对派组织构成威胁。\n",
      "\n",
      "声明\t说\t，联军随即对该武装力量进行了轰炸，炸毁了对方的大炮和防空武器。\n",
      "\n",
      "文章\t称\t，《经济参考报》记者从多家社会监测机构了解到，近期国际原油价格受诸多利空消息影响持续走低，国内参考的原油变化率持续负值内下滑，本周五（6月9日）国内成品油零售限价大概率迎来下调，无缘“两连涨”。\n",
      "\n",
      "卓创资讯成品油分析师王芦青\t认为\t，调价窗口开启前，国际原油消息面缺乏相应的利好支撑，油价出现大幅上涨的可能不大，因而本周五24时，国内成品油零售限价迎来下调或成大概率事件。\n",
      "\n",
      "中宇资讯分析师许磊\t表示\t，近一个月以来，全国两桶油加油站大打价格战，促销优惠幅度在1－1.6元／升不等，以提高零售份额，对于民营加油站造成较大冲击，终端消费资源不断减少。\n",
      "\n",
      "业内人士\t表示\t，受到调控政策的影响，以及银行出于自身资产负债调整的需要，今年银行房贷或出现全面“量缩价升”的情形。\n",
      "\n",
      "伟嘉安捷企划经理吴昊\t说\t，在政策较严的2010年和2014年，首套房贷款利率都出现过上浮1.1倍和1.15倍的情况。\n",
      "\n",
      "北京的林先生\t告诉\t记者，这两天，他和买家一直都在联系其他银行，但均被告知“额度需要排队”。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s, v, p in zip(subs, says, points): \n",
    "    print(s + '\\t' + v + '\\t' + p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
