{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T17:22:29.856194Z",
     "start_time": "2019-08-27T17:22:28.504835Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('news_chinese.csv')\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 获得所有表示“说”的意思的单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T16:56:04.396204Z",
     "start_time": "2019-08-27T16:56:04.197205Z"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "LTP_DATA_DIR = '.\\\\ltp_data_v3.4.0\\\\'  # ltp模型目录的路径\n",
    "cws_model_path = os.path.join(LTP_DATA_DIR, 'cws.model')  # 分词模型路径，模型名称为`cws.model`\n",
    "\n",
    "from pyltp import Segmentor\n",
    "segmentor = Segmentor()  # 初始化实例\n",
    "segmentor.load(cws_model_path)  # 加载模型\n",
    "\n",
    "def cut_words(text):\n",
    "    words = segmentor.segment(text)  # 分词\n",
    "    return words\n",
    "#     segmentor.release()  # 释放模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T17:20:02.262676Z",
     "start_time": "2019-08-27T17:20:02.257707Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyltp import SentenceSplitter\n",
    "\n",
    "def split_sentence(text):\n",
    "    '''\n",
    "    split text into single sentence without '\\n'\n",
    "    '''\n",
    "    sentences = []\n",
    "    try:\n",
    "        sentence_list = text.split('\\\\n')\n",
    "        for sentence in sentence_list:\n",
    "            splited_sentence = SentenceSplitter.split(sentence)  # 分句\n",
    "            sentences.extend(list(splited_sentence))\n",
    "    except Exception:\n",
    "        print('split_sentence exception:' + str(text))\n",
    "        print(Exception)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T17:00:52.305342Z",
     "start_time": "2019-08-27T17:00:52.301341Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    preprocess text, drop number, blank, stopwords\n",
    "    return segments list\n",
    "    \"\"\"\n",
    "#     stopwords=pd.read_csv('.\\\\stopwords.txt',index_col=False,quoting=3,sep=\"\\t\",names=['stopword'], encoding='utf-8')\n",
    "#     stopwords=stopwords['stopword'].values\n",
    "    if len(text) == 1:\n",
    "        return ''\n",
    "    \n",
    "    try:\n",
    "        segs = list(cut_words(text))\n",
    "#         segs = [v for v in segs if not str(v).isdigit()]#去数字\n",
    "        segs = list(filter(lambda x:x.strip(), segs)) #去左右空格\n",
    "#         segs = list(filter(lambda x:len(x)>1, segs))#长度为1的字符\n",
    "#         segs = list(filter(lambda x:x not in stopwords, segs)) #去掉停用词\n",
    "    except Exception:\n",
    "        print('preprocess_text exception:' + str(text))\n",
    "        print(Exception)\n",
    "    return segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T17:25:20.492826Z",
     "start_time": "2019-08-27T17:22:50.281775Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "preprocess and write corpus to file for future use\n",
    "'''\n",
    "with open('corpus.txt', 'w', encoding='utf-8') as f:\n",
    "    for graph in data['content'].values:\n",
    "        sentences = split_sentence(graph)  # 分句\n",
    "        for sentence in sentences:\n",
    "            words = preprocess_text(sentence)\n",
    "            if words == '':\n",
    "                continue\n",
    "            f.write(' '.join(words))\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T17:44:09.938215Z",
     "start_time": "2019-08-27T17:41:54.444315Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\py36\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "E:\\Anaconda3\\envs\\py36\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "corpus = LineSentence('corpus.txt')\n",
    "'''\n",
    "LineSentence(inp)：格式简单：一句话=一行; 单词已经过预处理并被空格分隔。\n",
    "size：是每个词的向量维度； \n",
    "window：是词向量训练时的上下文扫描窗口大小，窗口为5就是考虑前5个词和后5个词； \n",
    "min-count：设置最低频率，默认是5，如果一个词语在文档中出现的次数小于5，那么就会丢弃； \n",
    "workers：是训练的进程数（需要更精准的解释，请指正），默认是当前运行机器的处理器核数。这些参数先记住就可以了。\n",
    "sg ({0, 1}, optional) – 模型的训练算法: 1: skip-gram; 0: CBOW\n",
    "alpha (float, optional) – 初始学习率\n",
    "iter (int, optional) – 迭代次数，默认为5\n",
    "'''\n",
    "model = Word2Vec(sentences=corpus, size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "model.save(\".\\\\word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.20866792,  0.7205171 , -1.0260113 , -0.06374749,  0.19556347,\n",
       "       -0.10682994, -0.5851775 , -0.01481899,  0.3670453 ,  0.24029326,\n",
       "        0.5949725 ,  0.28039786, -0.06872837, -0.33250424, -0.1844618 ,\n",
       "       -0.5172169 ,  0.34398785, -0.58655286, -0.1243936 , -0.75491464,\n",
       "        0.09888597,  0.40916127,  0.10244063,  0.7225233 ,  0.32497236,\n",
       "        0.56063825,  0.8444974 , -0.3222012 ,  0.34923565,  0.27028653,\n",
       "       -0.51589775, -0.5600994 ,  0.2264611 , -0.385016  , -0.4579577 ,\n",
       "        0.08523443,  0.64386386, -0.10020302, -0.01174884, -0.51918715,\n",
       "       -0.01156036, -0.22992441,  0.77415377,  0.05849807, -0.2401069 ,\n",
       "       -0.00274309, -0.7530591 ,  0.07110906, -0.13691097,  0.1307326 ,\n",
       "       -0.49360964, -0.21363465,  0.7133124 ,  0.357119  ,  0.47492936,\n",
       "       -0.23782545,  1.0265316 , -0.13638072, -0.27368957, -0.03718648,\n",
       "        0.7090336 , -0.76925814, -0.48834   ,  0.71585935,  0.19875243,\n",
       "       -0.85786104,  0.28460357, -0.29654166, -0.14113064,  0.8782624 ,\n",
       "       -0.11440696,  0.20456931, -0.24379306,  0.80188143,  0.19253168,\n",
       "        0.04655569, -0.82315147,  0.13960767,  0.06716272, -0.46724465,\n",
       "       -0.02533841, -0.47917455, -0.6083708 ,  0.75265986,  0.12438376,\n",
       "       -0.4198804 , -0.04766526,  0.38098052,  0.37672096,  0.706703  ,\n",
       "       -0.44198063, -0.8817988 , -0.20643437,  0.16330408,  0.51761544,\n",
       "       -0.37391335,  0.09321438,  0.38813108, -0.32184866, -0.31084642],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['说']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 找相似词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embedding Tutorial: word2vec using Gensim\n",
    "\n",
    "https://www.guru99.com/word-embedding-word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T03:08:45.471839Z",
     "start_time": "2019-09-11T03:08:35.226636Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\py36\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "E:\\Anaconda3\\envs\\py36\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "load gensim word2vec model\n",
    "'''\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\".\\\\word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T18:07:39.145355Z",
     "start_time": "2019-09-09T18:07:39.133334Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198596"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(model.wv.vocab)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T18:07:42.600970Z",
     "start_time": "2019-09-09T18:07:42.228263Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Soft\\Anaconda3\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('表示', 0.8741297125816345), ('告诉', 0.8271186351776123), ('指出', 0.788396954536438), ('认为', 0.7727850675582886), ('介绍', 0.7356007695198059), ('说道', 0.7067561745643616), ('看来', 0.6960127353668213), ('写道', 0.6913279891014099), ('话', 0.6870427131652832), ('强调', 0.6705794334411621), ('坦言', 0.6692733764648438), ('提到', 0.6580647230148315), ('称', 0.6515368223190308), ('透露', 0.6492000222206116), ('举例', 0.6212161779403687), ('形容', 0.611984372138977), ('证实', 0.6084652543067932), ('得知', 0.606695294380188), ('说法', 0.6029568910598755), ('问', 0.6021037101745605)]\n"
     ]
    }
   ],
   "source": [
    "similar_words = model.most_similar('说', topn=20)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T18:07:48.504598Z",
     "start_time": "2019-09-09T18:07:48.500620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide the similarity between these two words:\n",
      "0.3628039284301186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Soft\\Anaconda3\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "similarity_two_words = model.similarity('说','讲')\n",
    "print(\"Please provide the similarity between these two words:\")\n",
    "print(similarity_two_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T18:07:52.778618Z",
     "start_time": "2019-09-09T18:07:52.764648Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('表示', 0.8741297125816345), ('告诉', 0.8271186351776123), ('指出', 0.788396954536438), ('认为', 0.7727850675582886), ('介绍', 0.7356007695198059), ('说道', 0.7067561745643616), ('看来', 0.6960127353668213), ('写道', 0.6913279891014099), ('话', 0.6870427131652832), ('强调', 0.6705794334411621), ('坦言', 0.6692733764648438), ('提到', 0.6580647230148315), ('称', 0.6515368223190308), ('透露', 0.6492000222206116), ('举例', 0.6212161779403687), ('形容', 0.611984372138977), ('证实', 0.6084652543067932), ('得知', 0.606695294380188), ('说法', 0.6029568910598755), ('问', 0.6021037101745605)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Soft\\Anaconda3\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "similar = model.similar_by_word('说', topn=20)\n",
    "print(similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T03:09:22.115307Z",
     "start_time": "2019-09-11T03:09:22.111309Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_similar_words(words, max_size, model):\n",
    "    similar_words = []\n",
    "    seen = []\n",
    "    unseen = ['说']\n",
    "    while len(similar_words) < max_size and len(unseen) != 0:\n",
    "        unseen_word = unseen.pop(0)\n",
    "        if unseen_word in similar_words:\n",
    "            continue\n",
    "        similars = [w for w, p in model.most_similar(unseen_word, topn=20)]\n",
    "        unseen.extend(similars)\n",
    "        similar_words.append(unseen_word)\n",
    "        seen.append(unseen_word)\n",
    "    return similar_words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T03:11:58.811192Z",
     "start_time": "2019-09-11T03:09:31.446014Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\vick_\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.977 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "words = []\n",
    "with open('corpus.txt', 'r', encoding='utf-8') as rf:\n",
    "    for line in rf.readlines():\n",
    "        words.extend(list(jieba.cut(line)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T18:12:44.824969Z",
     "start_time": "2019-09-09T18:12:44.819997Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33172819"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T03:19:12.598287Z",
     "start_time": "2019-09-11T03:19:11.717303Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['说',\n",
       " '表示',\n",
       " '告诉',\n",
       " '指出',\n",
       " '认为',\n",
       " '说道',\n",
       " '坦言',\n",
       " '介绍',\n",
       " '看来',\n",
       " '透露',\n",
       " '写道',\n",
       " '举例',\n",
       " '强调',\n",
       " '深有感触',\n",
       " '称',\n",
       " '话',\n",
       " '直言',\n",
       " '说法',\n",
       " '提到',\n",
       " '笑言']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words = find_similar_words(words, 20, model)\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 使用 NER，Dependency Parsing等对句子形式进行解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T08:36:01.148092Z",
     "start_time": "2019-09-11T08:35:59.864821Z"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "LTP_DATA_DIR = '.\\\\ltp_data_v3.4.0\\\\'  # ltp模型目录的路径\n",
    "pos_model_path = os.path.join(LTP_DATA_DIR, 'pos.model')  # 词性标注模型路径，模型名称为`pos.model`\n",
    "ner_model_path = os.path.join(LTP_DATA_DIR, 'ner.model')  # 命名实体识别模型路径，模型名称为`pos.model`\n",
    "par_model_path = os.path.join(LTP_DATA_DIR, 'parser.model')  # 依存句法分析模型路径，模型名称为`parser.model`\n",
    "srl_model_path = os.path.join(LTP_DATA_DIR, 'pisrl_win.model')  # 语义角色标注模型目录路径，模型目录为`srl`。注意该模型路径是一个目录，而不是一个文件。\n",
    "\n",
    "from pyltp import Postagger\n",
    "postagger = Postagger() # 初始化实例\n",
    "postagger.load(pos_model_path)  # 加载模型\n",
    "\n",
    "from pyltp import NamedEntityRecognizer\n",
    "recognizer = NamedEntityRecognizer() # 初始化实例\n",
    "recognizer.load(ner_model_path)  # 加载模型\n",
    "\n",
    "from pyltp import Parser\n",
    "parser = Parser() # 初始化实例\n",
    "parser.load(par_model_path)  # 加载模型\n",
    "\n",
    "from pyltp import SementicRoleLabeller\n",
    "labeller = SementicRoleLabeller() # 初始化实例\n",
    "labeller.load(srl_model_path)  # 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-11T09:53:37.593Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "maxsize = 30\n",
    "count = 0\n",
    "\n",
    "with open('corpus.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        words = line.split(' ')\n",
    "        keyword = contain_keywords(similar_words, words) # 句子是否包含“说”的同义词\n",
    "        if keyword == '':\n",
    "            continue\n",
    "        print('%d:%s' % (words.index(keyword), keyword))\n",
    "        postags = postagger.postag(words)  # 词性标注\n",
    "        netags = recognizer.recognize(words, postags)  # 命名实体识别\n",
    "        arcs = parser.parse(words, postags)  # 句法分析\n",
    "        \n",
    "        # 打印词性标注，命名实体识别，句法分析结果(可注释)\n",
    "        print('序号' + '\\t' + '词' + '\\t' + '词性' + '\\t' + '命名实体' + '\\t' + '依存句法')\n",
    "        print('--------------------------------------------')\n",
    "        word_index = 0\n",
    "        for w,p,n,a in zip(words, postags, netags, (\"%d:%s\" % (arc.head, arc.relation) for arc in arcs)):\n",
    "            print(str(word_index) + '\\t' + w + '\\t' + p + '\\t' + n + '\\t' + a)\n",
    "            word_index += 1\n",
    "        \n",
    "        # arcs 使用依存句法分析的结果\n",
    "        roles = labeller.label(words, postags, arcs)  # 语义角色标注\n",
    "        \n",
    "        # 打印依存句法分析结果(可注释)\n",
    "        for role in roles:\n",
    "            print(role.index, \"\".join(\n",
    "                [\"%s:(%d,%d)\" % (arg.name, arg.range.start, arg.range.end) for arg in role.arguments]))\n",
    "        \n",
    "        for role in roles:\n",
    "            if words.index(keyword) == role.index:\n",
    "                for arg in role.arguments:\n",
    "                    if arg.name == 'A0':\n",
    "                        sub = ''.join(words[arg.range.start:arg.range.end])\n",
    "                        print(\"观点主体\" + sub)\n",
    "        \n",
    "        print('\\n======================================================================\\n')\n",
    "        count += 1\n",
    "        if count == maxsize:\n",
    "            break\n",
    "\n",
    "postagger.release()  # 释放模型\n",
    "recognizer.release()  # 释放模型\n",
    "parser.release()  # 释放模型\n",
    "labeller.release()  # 释放模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T03:30:01.286025Z",
     "start_time": "2019-09-11T03:30:01.282040Z"
    }
   },
   "outputs": [],
   "source": [
    "def contain_keywords(keywords, words):\n",
    "    '''\n",
    "    return key word in words, return empty if not match\n",
    "    '''\n",
    "    for keyword in keywords:\n",
    "        if keyword in words:\n",
    "            return keyword\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 命名实体识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T03:06:02.505982Z",
     "start_time": "2019-09-11T03:06:02.156373Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S-Ni\tB-Ns\tE-Ns\tO\tO\tO\tO\tO\tO\tO\tO\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "LTP_DATA_DIR = '.\\\\ltp_data_v3.4.0\\\\'  # ltp模型目录的路径\n",
    "ner_model_path = os.path.join(LTP_DATA_DIR, 'ner.model')  # 命名实体识别模型路径，模型名称为`pos.model`\n",
    "\n",
    "from pyltp import NamedEntityRecognizer\n",
    "recognizer = NamedEntityRecognizer() # 初始化实例\n",
    "recognizer.load(ner_model_path)  # 加载模型\n",
    "\n",
    "# words = ['元芳', '你', '怎么', '看']\n",
    "# postags = ['nh', 'r', 'r', 'v']\n",
    "netags = recognizer.recognize(words, postags)  # 命名实体识别\n",
    "\n",
    "print('\\t'.join(netags))\n",
    "recognizer.release()  # 释放模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
