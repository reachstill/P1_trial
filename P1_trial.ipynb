{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T17:22:29.856194Z",
     "start_time": "2019-08-27T17:22:28.504835Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('news_chinese.csv')\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 获得所有表示“说”的意思的单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T16:56:04.396204Z",
     "start_time": "2019-08-27T16:56:04.197205Z"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "LTP_DATA_DIR = '.\\\\ltp_data_v3.4.0\\\\'  # ltp模型目录的路径\n",
    "cws_model_path = os.path.join(LTP_DATA_DIR, 'cws.model')  # 分词模型路径，模型名称为`cws.model`\n",
    "\n",
    "from pyltp import Segmentor\n",
    "segmentor = Segmentor()  # 初始化实例\n",
    "segmentor.load(cws_model_path)  # 加载模型\n",
    "\n",
    "def cut_words(text):\n",
    "    words = segmentor.segment(text)  # 分词\n",
    "    return words\n",
    "#     segmentor.release()  # 释放模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T17:20:02.262676Z",
     "start_time": "2019-08-27T17:20:02.257707Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyltp import SentenceSplitter\n",
    "\n",
    "def split_sentence(text):\n",
    "    '''\n",
    "    split text into single sentence without '\\n'\n",
    "    '''\n",
    "    sentences = []\n",
    "    try:\n",
    "        sentence_list = text.split('\\\\n')\n",
    "        for sentence in sentence_list:\n",
    "            splited_sentence = SentenceSplitter.split(sentence)  # 分句\n",
    "            sentences.extend(list(splited_sentence))\n",
    "    except Exception:\n",
    "        print('split_sentence exception:' + str(text))\n",
    "        print(Exception)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T17:00:52.305342Z",
     "start_time": "2019-08-27T17:00:52.301341Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    preprocess text, drop number, blank, stopwords\n",
    "    return segments list\n",
    "    \"\"\"\n",
    "#     stopwords=pd.read_csv('.\\\\stopwords.txt',index_col=False,quoting=3,sep=\"\\t\",names=['stopword'], encoding='utf-8')\n",
    "#     stopwords=stopwords['stopword'].values\n",
    "    if len(text) == 1:\n",
    "        return ''\n",
    "    \n",
    "    try:\n",
    "        segs = list(cut_words(text))\n",
    "#         segs = [v for v in segs if not str(v).isdigit()]#去数字\n",
    "        segs = list(filter(lambda x:x.strip(), segs)) #去左右空格\n",
    "#         segs = list(filter(lambda x:len(x)>1, segs))#长度为1的字符\n",
    "#         segs = list(filter(lambda x:x not in stopwords, segs)) #去掉停用词\n",
    "    except Exception:\n",
    "        print('preprocess_text exception:' + str(text))\n",
    "        print(Exception)\n",
    "    return segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T17:25:20.492826Z",
     "start_time": "2019-08-27T17:22:50.281775Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "preprocess and write corpus to file for future use\n",
    "'''\n",
    "with open('corpus.txt', 'w', encoding='utf-8') as f:\n",
    "    for graph in data['content'].values:\n",
    "        sentences = split_sentence(graph)  # 分句\n",
    "        for sentence in sentences:\n",
    "            words = preprocess_text(sentence)\n",
    "            if words == '':\n",
    "                continue\n",
    "            f.write(' '.join(words))\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T17:44:09.938215Z",
     "start_time": "2019-08-27T17:41:54.444315Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Soft\\Anaconda3\\envs\\py36\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "corpus = LineSentence('corpus.txt')\n",
    "'''\n",
    "LineSentence(inp)：格式简单：一句话=一行; 单词已经过预处理并被空格分隔。\n",
    "size：是每个词的向量维度； \n",
    "window：是词向量训练时的上下文扫描窗口大小，窗口为5就是考虑前5个词和后5个词； \n",
    "min-count：设置最低频率，默认是5，如果一个词语在文档中出现的次数小于5，那么就会丢弃； \n",
    "workers：是训练的进程数（需要更精准的解释，请指正），默认是当前运行机器的处理器核数。这些参数先记住就可以了。\n",
    "sg ({0, 1}, optional) – 模型的训练算法: 1: skip-gram; 0: CBOW\n",
    "alpha (float, optional) – 初始学习率\n",
    "iter (int, optional) – 迭代次数，默认为5\n",
    "'''\n",
    "model = Word2Vec(sentences=corpus, size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "model.save(\".\\\\word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
